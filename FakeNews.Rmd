---
title: "FakeNews"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introdução

Nos foi introduzido, para a realização do trabalho, um dataset contendo dados de 30.000 estudantes aleatórios do ensino médio americano que possuem contas em uma rede social durante o ano de 2006. O nome da rede social foi mantido secreto para preservar a privacidade dos estudantes). Nosso objetivo com esse dataset era realizar a clusterização baseado nos conteúdos dos posts dos usuários.

# Método Utilizado

## Preparação dos dados

O primeiro foi realizar a leitura dos dados a partir do CSV e entender como estavam organizados.

```{r}
dataset <- read.csv('snsdata.csv')
table(dataset$bible)
table(dataset$basketball)
table(dataset$sexy)
table(dataset$rock)
table(dataset$dress)
table(dataset$death)
table(dataset$drugs)
summary(dataset$drugs)
summary(log(dataset$drugs))
summary(dataset$age)
summary(dataset$friends)
table(dataset$gender)
table(dataset$gradyear)
```

Os próximos passos da preparação foram: retirar os NA's, substituir os fatores do atributo "gender" para valores binários (fizemos isso para conseguirmos usar o kmeans futuramente),

```{r}
dataset <-  na.omit(dataset)
dataset$gender <- ifelse(dataset$gender=='M', 1, 0)
```

O último passo da preparação dos dados foi realizar a normalização. Utilizamos o mesmo método de normalização ensinado em aula.

```{r}
standardization <- function(x) {
  return ((x - mean(x)) / sd(x))
}
dataset_norm <- as.data.frame(lapply(dataset, standardization))
```

## Criação do Modelo

Com a normalização feita, conseguimos criar o modelo usando o kmeans. 

```{r}
model <- kmeans(dataset_norm, centers = 4)
```

